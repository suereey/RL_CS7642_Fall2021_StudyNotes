# Smoov & Curly's Bogus Journey

## Decision making & Reinforcement Learning
- Supervised learning
- Unsupervised learning 
- Reinforcement learning: give x and z, learn some function that can generate y.
- ![learnings_p1](https://raw.githubusercontent.com/suereey/RL_CS7642_Fall2021_StudyNotes/main/screenshot/P1_01.PNG)

## The world Example
- Steps to achieve goal
	- ![p2](https://raw.githubusercontent.com/suereey/RL_CS7642_Fall2021_StudyNotes/main/screenshot/P1_02.PNG)
- What if uncertainty invovles?
	- ![p3](https://raw.githubusercontent.com/suereey/RL_CS7642_Fall2021_StudyNotes/main/screenshot/P1_03.PNG)
- Markov Decision Processes
	- Markov property is only th epresent matters || Things are stationary.
	- State: S
	- Model: T(s,a,s')~ Pr(s'|a,s')
		- A transition model describes: the **rules** of the game that you are playing in the **physics of the world**.
		- The T function prduces the probability that you will end up trasitioning the state s', given that you were in state s and you took action a.
	- Actions: A(s), A 
		- up, down, left, right
	- Reward: **R(s)**, R(s,a), R(s,a,s')
		- Reward is a scalar value that you get for being in a state.
	- Policy:
		- The above describes a problem, and **policy** is the solution.
		- π(s) → a (what policy does is: it's a function that takes in a state, and returns an action.)
		- π* (optimal policy: the policy that maximize your long-term expected reward.)
	- ![p4](https://raw.githubusercontent.com/suereey/RL_CS7642_Fall2021_StudyNotes/main/screenshot/P1_04.PNG)

## MDPs: More about Reward
- Delayed reward
	- ![p5](https://raw.githubusercontent.com/suereey/RL_CS7642_Fall2021_StudyNotes/main/screenshot/P1_05.PNG)
- Minor changes matter: the value of R(s) matters.
	- ![p6](https://raw.githubusercontent.com/suereey/RL_CS7642_Fall2021_StudyNotes/main/screenshot/P1_06.PNG)

## Sequnce of reward: assmptions
- ![p7](https://raw.githubusercontent.com/suereey/RL_CS7642_Fall2021_StudyNotes/main/screenshot/P1_07.PNG)
- ![p8](https://raw.githubusercontent.com/suereey/RL_CS7642_Fall2021_StudyNotes/main/screenshot/P1_08.PNG)
- ![p9](https://raw.githubusercontent.com/suereey/RL_CS7642_Fall2021_StudyNotes/main/screenshot/P1_09.PNG)
## Assumptions
- ![p10](https://raw.githubusercontent.com/suereey/RL_CS7642_Fall2021_StudyNotes/main/screenshot/P1_10.PNG)

## Policies (important)
- Bellman equation
	- ![p11]()


